# -*- coding: utf-8 -*-
"""Our Projectt.ipynb adlÄ± not defterinin kopyasÄ±

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jo467uJ6y7fXCEb4qC4gIPb1ZCZ6q5gp
"""

import os
import shutil
import numpy as np
import pandas as pd
import random
import fnmatch
import math
import cv2
import matplotlib.pyplot as plt
import seaborn as sns

# Makine Ã–ÄŸrenmesi (ML) ve DeÄŸerlendirme iÃ§in gerekli kÃ¼tÃ¼phaneler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.utils import class_weight

# Derin Ã–ÄŸrenme (DL) ve Keras kÃ¼tÃ¼phaneleri
import tensorflow as tf
from keras.utils import to_categorical
from keras.models import Model, load_model
from keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications import VGG16, ResNet50, InceptionV3, EfficientNetB0

print("KÃ¼tÃ¼phaneler yÃ¼klendi.")

IMG_WIDTH = 96  # GÃ¶rÃ¼ntÃ¼ geniÅŸliÄŸi
IMG_HEIGHT = 96 # GÃ¶rÃ¼ntÃ¼ yÃ¼ksekliÄŸi
CHANNELS = 3    # GÃ¶rÃ¼ntÃ¼ kanal sayÄ±sÄ± (RGB iÃ§in 3)
NUM_CLASSES = 2 # SÄ±nÄ±f sayÄ±sÄ± (Kansersiz/Kanserli)
TEST_RATIO = 0.2 # Test veri setinin oranÄ±
VALIDATION_RATIO = 0.1 # DoÄŸrulama veri setinin oranÄ±
NUM_PATIENT_FOLDERS_TO_COPY = 10
LOCAL_DIR = '/content/local_data/veriSeti' # Yerel veri seti yolu
MODEL_DIR = '/content/models' # Modellerin kaydedileceÄŸi dizin

print("\nGoogle Drive baÄŸlantÄ±sÄ± baÅŸlatÄ±lÄ±yor...")
from google.colab import drive
drive.mount('/content/drive')
BASE_DIR = '/content/drive/MyDrive/Projects/YapayZeka/veriSeti' # Veri setinin Google Drive'daki ana dizini
SOURCE_DIR = BASE_DIR # Kaynak dizin olarak BASE_DIR'Ä± kullan
os.makedirs(MODEL_DIR, exist_ok=True) # Modeller iÃ§in dizin oluÅŸtur, zaten varsa hata verme
print("Google Drive baÅŸarÄ±yla baÄŸlandÄ±.")

LOCAL_DIR = '/content/local_data/' # Yerel veri dizini yolu
if os.path.exists(LOCAL_DIR): # EÄŸer yerel veri dizini varsa
    shutil.rmtree(LOCAL_DIR) # Dizini ve iÃ§eriÄŸini sil
    print(f"'{LOCAL_DIR}' baÅŸarÄ±yla silindi.")
else:
    print(f"'{LOCAL_DIR}' klasÃ¶rÃ¼ bulunamadÄ±.")

MODEL_DIR = '/content/models' # Modellerin kaydedileceÄŸi dizin yolu
if os.path.exists(MODEL_DIR): # EÄŸer model dizini varsa
  shutil.rmtree(MODEL_DIR) # Dizini ve iÃ§eriÄŸini sil
  print(f"'{MODEL_DIR}' baÅŸarÄ±yla silindi.")
else:
  print(f"'{MODEL_DIR}' klasÃ¶rÃ¼ bulunamadÄ±.")

print("\n2. Veri Edinimi")

def copy_and_load_paths(source_dir, local_dir, num_patients):
    """Veriyi kopyalar ve etiketli yollarÄ± dÃ¶ndÃ¼rÃ¼r."""
    # Yerel dizini temizle veya oluÅŸtur
    if os.path.exists(local_dir): shutil.rmtree(local_dir)
    os.makedirs(local_dir, exist_ok=True)

    try:
        # TÃ¼m hasta dizinlerini al ve 'test' klasÃ¶rÃ¼nÃ¼ hariÃ§ tut
        all_patient_dirs = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]
        if 'sonTest' in all_patient_dirs: all_patient_dirs.remove('sonTest')

        # Rastgele hasta dizinlerini seÃ§
        selected_patient_dirs = random.sample(all_patient_dirs, min(len(all_patient_dirs), num_patients))
        print(f"Toplam {len(all_patient_dirs)} hasta bulundu. Rastgele {len(selected_patient_dirs)} eÄŸitim hastasÄ± seÃ§ildi.")

        # SeÃ§ilen hasta dizinlerini yerel dizine kopyala
        for p_dir in selected_patient_dirs:
            shutil.copytree(os.path.join(source_dir, p_dir), os.path.join(local_dir, p_dir))

        # Etiketsiz 'sonTest' klasÃ¶rÃ¼nÃ¼ de driveden al
        source_test_dir = os.path.join(BASE_DIR, 'sonTest')
        local_test_dir = os.path.join(LOCAL_DIR, 'sonTest')
        if os.path.exists(source_test_dir):
            shutil.copytree(source_test_dir, local_test_dir)
            print("Etiketsiz 'sonTest' klasÃ¶rÃ¼ de kopyalandÄ±.")
    except Exception as e:
        print(f"Kopyalama HATA: {e}. Kod devam edemeyebilir.")
        return []

    # TÃ¼m slayt parÃ§alarÄ±nÄ±n yollarÄ±nÄ± topla
    all_patches = []
    for root, _, filenames in os.walk(local_dir):
        for filename in fnmatch.filter(filenames, '*.png'):
            all_patches.append(os.path.join(root, filename))

    # SonTest harici tÃ¼m png dosyalarÄ±nÄ±n yolunu kopyala
    train_only_paths = [p for p in all_patches if '/sonTest/' not in p] # 'sonTest' klasÃ¶rÃ¼ndekileri hariÃ§ tut
    print(f"Toplam Etiketli EÄŸitim GÃ¶rÃ¼ntÃ¼sÃ¼ SayÄ±sÄ±: **{len(train_only_paths)}**")
    return train_only_paths

# Veriyi kopyalalayan ve train gÃ¶rÃ¼ntÃ¼ yollarÄ±nÄ± tanÄ±mlayan fonksiyonu Ã§alÄ±ÅŸtÄ±r
train_only_paths = copy_and_load_paths(SOURCE_DIR, LOCAL_DIR, NUM_PATIENT_FOLDERS_TO_COPY)

# VERÄ° HAZIRLIÄI
print("\n## 3. Veri HazÄ±rlÄ±ÄŸÄ± ve BÃ¶lÃ¼mleme ğŸ—ï¸")

# Ham gÃ¶rÃ¼ntÃ¼ yollarÄ±nÄ± alÄ±p iÅŸler ve sayÄ±sal deÄŸerlere Ã§evirir
def load_and_prepare_data(image_paths, width, height):
    """TÃ¼m gÃ¶rÃ¼ntÃ¼leri yÃ¼kler, normalize eder ve etiketleri Ã§Ä±karÄ±r."""
    X_data = []; Y_labels = []
    print(f"TÃ¼m {len(image_paths)} etiketli gÃ¶rÃ¼ntÃ¼ RAM'e yÃ¼kleniyor ayrÄ±ca gÃ¶rÃ¼ntÃ¼ler {width}x{height} boyutuna iÅŸleniyor...")
    for i, img_path in enumerate(image_paths):
        if i % 5000 == 0 and i > 0: print(f"    --> {i} gÃ¶rÃ¼ntÃ¼ yÃ¼klendi...")
        img = cv2.imread(img_path) # GÃ¶rÃ¼ntÃ¼yÃ¼ oku ve piksel verileri belleÄŸe al
        if img is None: continue # GÃ¶rÃ¼ntÃ¼ okunamadÄ±ysa atla
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR'den RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r
        img_resized = cv2.resize(img_rgb, (width, height), interpolation=cv2.INTER_CUBIC) # BoyutlandÄ±r
        X_data.append(img_resized / 255.0) # piksel verilerini NormalleÅŸtir ve listeye ekle
        Y_labels.append(int(img_path.split('class')[-1][0])) # Etiketi al
    return np.array(X_data, dtype='float32'), np.array(Y_labels)

# GÃ¶rÃ¼ntÃ¼leri yÃ¼kle ve hazÄ±rla
X, Y = load_and_prepare_data(train_only_paths, IMG_WIDTH, IMG_HEIGHT)

# SÄ±nÄ±f sayÄ±mlarÄ±nÄ± bul
count_zero = np.sum(Y == 0)
count_one = np.sum(Y == 1)

# Pasta grafiÄŸi Ã§iz
plt.figure(figsize=(7, 6))
plt.pie([count_zero, count_one], labels=[f'IDC- ({count_zero})', f'IDC+ ({count_one})'],
        colors=['#66b3ff','#ff9999'], autopct='%1.1f%%', startangle=90)
plt.title('SÄ±nÄ±f OranlarÄ± (TÃ¼m EÄŸitim Verisi)'); plt.tight_layout(); plt.show()

# Dengesizlik oranÄ±nÄ± yazdÄ±r
print(f"\nGÃ¶rÃ¼ntÃ¼lerdeki Dengesizlik OranÄ±!!!: 1:{count_zero/count_one:.2f}")

# VERÄ° BÃ–LÃœMLEME
# stratify = Y parametresi, 0 ve 1 lerin eÄŸitim, val ve testte de aynÄ± oranda olmasÄ±nÄ± saÄŸlar
X_train, X_rem, Y_train, Y_rem = train_test_split(X, Y, test_size=TEST_RATIO + VALIDATION_RATIO, random_state=42, stratify=Y)
X_val, X_test, Y_val, Y_test = train_test_split(X_rem, Y_rem, test_size=TEST_RATIO / (TEST_RATIO + VALIDATION_RATIO), random_state=42, stratify=Y_rem)

# Etiketleri one-hot encoding formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r (1 sÄ±nÄ±fÄ± 0 dan Ã¼stÃ¼n deÄŸildir)
Y_trainHot, Y_valHot, Y_testHot = to_categorical(Y_train, NUM_CLASSES), to_categorical(Y_val, NUM_CLASSES), to_categorical(Y_test, NUM_CLASSES)

# SÄ±nÄ±f DengesizliÄŸini gidermek iÃ§in SÄ±nÄ±f AÄŸÄ±rlÄ±klarÄ± hesapla
class_weights_full = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train)
class_weight_dict = dict(enumerate(class_weights_full))
print(f"EÄŸitim Verisi: {len(X_train)} | SÄ±nÄ±f AÄŸÄ±rlÄ±klarÄ±: {class_weight_dict}")
# Bellekten yer aÃ§mak iÃ§in orijinal veriyi sil
del X, Y

# Veri ArtÄ±rma (Data Augmentation) TanÄ±mÄ±
datagen = ImageDataGenerator(
    rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,
    horizontal_flip=True, vertical_flip=True, zoom_range=0.1, fill_mode='nearest'
)
datagen.fit(X_train) # EÄŸitim verisi Ã¼zerinde veri arttÄ±rma yapÄ±lcak

print("\n## 4. Ã‡oklu Model TanÄ±mÄ± ğŸ§ ")

# Derin Ã¶ÄŸrenme Modellerimizin ayarlarÄ± deÄŸiÅŸir (giriÅŸ: temel modelin giriÅŸi, Ã§Ä±kÄ±ÅŸ: yeni  katmanlar)
def build_custom_head(base_model, num_classes=2):
    # Temel modelin Ã§Ä±kÄ±ÅŸ katmanÄ±nÄ± al
    x = base_model.output
    # Parametreleri azlatmak iÃ§in deÄŸiÅŸkenlerin ortalamasÄ± alÄ±nÄ±r.
    x = GlobalAveragePooling2D(name='global_avg_pool')(x)

    # Tam BaÄŸlantÄ±lÄ± (Fully Connected-aktivasyon fonk.) katman ekle
    x = Dense(512, activation='relu', name='fc1_custom')(x)
    x = Dropout(0.5, name='dropout_1')(x) # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi engellemek iÃ§in Dropout
    x = Dense(256, activation='relu', name='fc2_custom')(x)
    x = Dropout(0.4, name='dropout_2')(x)
    # Ã‡Ä±kÄ±ÅŸ katmanÄ± (softmax, Ã§oklu sÄ±nÄ±flandÄ±rma iÃ§in)
    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)
    return Model(inputs=base_model.input, outputs=predictions)

def define_model(model_name, input_shape, weights='imagenet'):
    """SeÃ§ilen mimariyi yÃ¼kler ve yeni temel modeli oluÅŸturur."""
    print(f"\n--- Model YÃ¼kleniyor: {model_name} ---")

    # Model ismine gÃ¶re Ã¶nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klarla temel modeli yÃ¼kle
    if model_name == 'VGG16':
        base = VGG16(weights=weights, include_top=False, input_tensor=Input(shape=input_shape))
        last_conv_layer_name = 'block5_conv3'
    elif model_name == 'ResNet50':
        base = ResNet50(weights=weights, include_top=False, input_tensor=Input(shape=input_shape))
        last_conv_layer_name = 'conv5_block3_out'
    else:
        raise ValueError(f"Bilinmeyen model: {model_name}")

    # Temel modelin Ã¼zerine Ã¶zel sÄ±nÄ±flandÄ±rma baÅŸlÄ±ÄŸÄ±nÄ± ekleyen fonk. Ã§alÄ±ÅŸtÄ±r
    model = build_custom_head(base, NUM_CLASSES)

    # Temel model katmanlarÄ±nÄ± dondur (Ã–znitelik Ã‡Ä±karma iÃ§in)
    for layer in base.layers:
        layer.trainable = False

    # Modeli derle Ã§alÄ±ÅŸÄ±r hale getir(loss fonksiyonu, optimizer ve metrikler belirtilir)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])
    return model, last_conv_layer_name

MODELS_TO_TRAIN = ['VGG16', 'ResNet50']
ALL_MODEL_RESULTS = {}

print("\n## 5. EÄŸitim ve Ä°nce Ayar DÃ¶ngÃ¼sÃ¼ BaÅŸlÄ±yor ğŸ”„")

BATCH_SIZE = 32 # Tek bir eÄŸitim adÄ±mÄ±nda iÅŸlenecek Ã¶rnek sayÄ±sÄ±
EPOCHS_FE = 35  # Ã–znitelik Ã‡Ä±karma aÅŸamasÄ±ndaki epoch sayÄ±sÄ±
EPOCHS_FT = 35  # Ä°nce Ayar aÅŸamasÄ±ndaki epoch sayÄ±sÄ±

# XLA_FLAGS'Ä± ayarlayarak CUDNN algoritma seÃ§imi hatalarÄ±nÄ± gÃ¶z ardÄ± et
os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'

# Belirlenen her bir model iÃ§in eÄŸitim ve ince ayar dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlat
for model_name in MODELS_TO_TRAIN:
    # Modeli tanÄ±mla ve son evriÅŸim katmanÄ±nÄ±n adÄ±nÄ± al
    model, last_conv_layer_name = define_model(model_name, (IMG_WIDTH, IMG_HEIGHT, CHANNELS))
    # Model aÄŸÄ±rlÄ±klarÄ±nÄ±n kaydedileceÄŸi yollarÄ± belirle
    model_save_path_fe = os.path.join(MODEL_DIR, f'{model_name}_fe_best.h5')
    model_save_path_ft = os.path.join(MODEL_DIR, f'{model_name}_ft_best.h5')

    print(f"\n[{model_name}] --- 5.1. Temel EÄŸitim (Feature Extraction) ---")

    # - Temel EÄŸitim (Ã–znitelik Ã‡Ä±karma) Temel model katmanlarÄ± dondurulur, sadece Ã¼st katmanlar eÄŸitilir
    # Modelin en iyi aÄŸÄ±rlÄ±klarÄ±nÄ± kaydetmek iÃ§in ModelCheckpoint callback'i
    checkpoint_fe = ModelCheckpoint(model_save_path_fe, monitor='val_accuracy', save_best_only=True, mode='max', verbose=0)
    # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi durdurmak iÃ§in EarlyStopping callback'i
    early_stopping_fe = EarlyStopping(monitor='val_loss', patience=12, mode='min', verbose=1)

    history_fe = model.fit(
        datagen.flow(X_train, Y_trainHot, batch_size=BATCH_SIZE), # Veri artÄ±rma ile eÄŸitim verisi akÄ±ÅŸÄ±
        steps_per_epoch=math.ceil(len(X_train) / BATCH_SIZE), # Her epoch'ta kaÃ§ adÄ±m Ã§alÄ±ÅŸÄ±lacaÄŸÄ±nÄ± belirle
        epochs=EPOCHS_FE, validation_data=(X_val, Y_valHot), # Epoch sayÄ±sÄ± ve doÄŸrulama verisi
        class_weight=class_weight_dict, # SÄ±nÄ±f dengesizliÄŸini gidermek iÃ§in sÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±
        callbacks=[checkpoint_fe, early_stopping_fe],
        verbose=1 # EÄŸitimi konsola yazdÄ±r
    )

    try: model = load_model(model_save_path_fe) # En iyi Ã¶znitelik Ã§Ä±karma modelini yÃ¼kle
    except: pass


    print(f"\n[{model_name}] --- 5.2. Ä°nce Ayar (Fine-Tuning) ---")

    # - Ä°nce Ayar HazÄ±rlÄ±ÄŸÄ± Modelin tamamÄ± veya belirli katmanlarÄ± eÄŸitilebilir hale getirilir
    model.trainable = True # Modelin tÃ¼m katmanlarÄ±nÄ± eÄŸitilebilir yap

    for layer in model.layers:
        # BazÄ± ilk konvolÃ¼syon bloklarÄ±nÄ± dondur, geri kalanlarÄ± aÃ§
        if model_name == 'VGG16' and layer.name.startswith('block1'): layer.trainable = False
        elif model_name == 'ResNet50' and layer.name.startswith('conv1'): layer.trainable = False
        # Ã–zel katmanlarÄ± (fc, dropout, global pooling) her zaman aÃ§Ä±k tut
        elif layer.name.startswith('fc') or layer.name.startswith('dropout') or layer.name.startswith('global'):
             layer.trainable = True
        else:
            layer.trainable = True # Kalan tÃ¼m evriÅŸimsel katmanlar ince ayar iÃ§in aÃ§Ä±lÄ±r.

    # Modeli yeniden derle

    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])

    # --- Ä°nce Ayar EÄŸitimi ---
    # Ä°nce ayar sonrasÄ± en iyi aÄŸÄ±rlÄ±klarÄ± kaydetmek iÃ§in ModelCheckpoint callback'i
    checkpoint_ft = ModelCheckpoint(model_save_path_ft, monitor='val_accuracy', save_best_only=True, mode='max', verbose=0)
    # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi durdurmak iÃ§in EarlyStopping callback'i
    early_stopping_ft = EarlyStopping(monitor='val_loss', patience=17, mode='min', verbose=1)

    history_ft = model.fit(
        datagen.flow(X_train, Y_trainHot, batch_size=BATCH_SIZE),
        steps_per_epoch=math.ceil(len(X_train) / BATCH_SIZE),
        epochs=EPOCHS_FT, validation_data=(X_val, Y_valHot),
        class_weight=class_weight_dict,
        callbacks=[checkpoint_ft, early_stopping_ft],
        verbose=1
    )

    # - Test ve SonuÃ§ KaydÄ±
    final_model = load_model(model_save_path_ft) # En iyi ince ayar modelini yÃ¼kle
    Y_pred = final_model.predict(X_test) # Test seti Ã¼zerinde tahmin yap
    Y_pred_classes = np.argmax(Y_pred, axis=1) # Tahmin edilen sÄ±nÄ±f etiketlerini olasÄ±lÄ±k ÅŸeklinde al
    Y_true_labels = np.argmax(Y_testHot, axis=1) # hot'tan gerÃ§eÄŸe Ã§evir

    # SÄ±nÄ±flandÄ±rma raporunu oluÅŸtur ve AUC deÄŸerini hesapla
    report = classification_report(Y_true_labels, Y_pred_classes, output_dict=True, zero_division=0)
    roc_auc = auc(roc_curve(Y_true_labels, Y_pred[:, 1])[0], roc_curve(Y_true_labels, Y_pred[:, 1])[1])

    # Model sonuÃ§larÄ±nÄ± kaydet
    ALL_MODEL_RESULTS[model_name] = {
        'Accuracy': report['accuracy'],
        'Recall_IDC+': report['1']['recall'],
        'Precision_IDC+': report['1']['precision'],
        'F1_IDC+': report['1']['f1-score'],
        'Recall_IDC-': report['0']['recall'],
        'Precision_IDC-': report['0']['precision'],
        'F1_IDC-': report['0']['f1-score'],
        'AUC': roc_auc,
        'Model_Path': model_save_path_ft,
        'Last_Conv_Layer': last_conv_layer_name,
        'History_FE': history_fe.history,
        'History_FT': history_ft.history
    }
    print(f"[{model_name}] Test SonuÃ§larÄ± Kaydedildi.")

def plot_history(history, model_name, phase):
    """EÄŸitim ve doÄŸrulama metriklerini grafikler."""
    plt.figure(figsize=(12, 5))

    # KayÄ±p (Loss) GrafiÄŸi
    plt.subplot(1, 2, 1)
    plt.plot(history['loss'], label='EÄŸitim KaybÄ±')
    plt.plot(history['val_loss'], label='DoÄŸrulama KaybÄ±')
    plt.title(f'{model_name} - {phase} KayÄ±p GrafiÄŸi')
    plt.xlabel('Epoch')
    plt.ylabel('KayÄ±p')
    plt.legend()
    plt.grid(True)

    # DoÄŸruluk (Accuracy) GrafiÄŸi
    plt.subplot(1, 2, 2)
    plt.plot(history['accuracy'], label='EÄŸitim DoÄŸruluÄŸu')
    plt.plot(history['val_accuracy'], label='DoÄŸrulama DoÄŸruluÄŸu')
    plt.title(f'{model_name} - {phase} DoÄŸruluk GrafiÄŸi')
    plt.xlabel('Epoch')
    plt.ylabel('DoÄŸruluk')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

print("\n--- EÄŸitim GeÃ§miÅŸi Grafikleri BaÅŸlÄ±yor ---")
for model_name, results in ALL_MODEL_RESULTS.items():
    if 'History_FE' in results:
        plot_history(results['History_FE'], model_name, "Ã–znitelik Ã‡Ä±karma (FE)")
    if 'History_FT' in results:
        plot_history(results['History_FT'], model_name, "Ä°nce Ayar (FT)")

print("\n## KarÅŸÄ±laÅŸtÄ±rmalÄ± DeÄŸerlendirme SonuÃ§larÄ± ğŸ“Š")

# TÃ¼m model sonuÃ§larÄ±nÄ± bir DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r ve AUC'ye gÃ¶re sÄ±rala
results_df = pd.DataFrame(ALL_MODEL_RESULTS).T.sort_values(by='AUC', ascending=False)
# En iyi modeli belirle
best_model_name = results_df.index[0]
best_model_path = ALL_MODEL_RESULTS[best_model_name]['Model_Path']
best_conv_layer = ALL_MODEL_RESULTS[best_model_name]['Last_Conv_Layer']

print("\n--- Ä°ki Model Performans Tablosu (VGG16 vs ResNet50) ---")
# ArtÄ±k 'Recall_IDC-', 'Precision_IDC-' ve 'F1_IDC-' metrikleri de tabloda yer almaktadÄ±r.
print(results_df.round(4))
print(f"\nEn YÃ¼ksek Performansa Sahip Model: **{best_model_name}** (AUC: {results_df['AUC'].max():.4f})")

# En iyi modelin yÃ¼klenmesi ve tahminlerin yeniden hesaplanmasÄ±
best_model = load_model(best_model_path)
Y_pred_best = best_model.predict(X_test, verbose=0) # Tahmin olasÄ±lÄ±klarÄ±
Y_pred_classes_best = np.argmax(Y_pred_best, axis=1) # Tahmin edilen sÄ±nÄ±f etiketleri
Y_true_labels = np.argmax(Y_testHot, axis=1) # GerÃ§ek sÄ±nÄ±f etiketleri

# Etiket haritasÄ± tanÄ±mla
map_characters = {0: 'IDC(-) Kansersiz', 1: 'IDC(+) Kanserli'}

# KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix) Ã§izimi
conf_mat = confusion_matrix(Y_true_labels, Y_pred_classes_best)
plt.figure(figsize = (7,7))
sns.heatmap(conf_mat, annot=True, fmt="d", cmap="YlGnBu",
             xticklabels=list(map_characters.values()),
             yticklabels=list(map_characters.values()))
plt.title(f'KarmaÅŸÄ±klÄ±k Matrisi ({best_model_name})'); plt.ylabel('GerÃ§ek Etiket'); plt.xlabel('Tahmin Edilen Etiket');
plt.show()

# Ã‡OKLU ROC EÄRÄ°SÄ° KARÅILAÅTIRMASI

plt.figure(figsize=(8, 8))
colors = ['darkorange', 'darkgreen', 'darkred', 'darkblue'] # VGG16 ve ResNet50 iÃ§in renkler

# Her bir model iÃ§in ROC eÄŸrisini Ã§izme dÃ¶ngÃ¼sÃ¼
for i, model_name in enumerate(MODELS_TO_TRAIN):
    model_path = ALL_MODEL_RESULTS[model_name]['Model_Path']

    try:
        current_model = load_model(model_path)
        # Tahmin olasÄ±lÄ±klarÄ± yeniden hesaplanÄ±yor
        Y_pred_current = current_model.predict(X_test, verbose=0)

        # ROC eÄŸrisi ve AUC deÄŸerini hesapla
        fpr, tpr, _ = roc_curve(Y_true_labels, Y_pred_current[:, 1])
        auc_score = ALL_MODEL_RESULTS[model_name]['AUC']

        # ROC eÄŸrisini Ã§iz
        plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,
                 label=f'{model_name} (AUC = {auc_score:.4f})')
    except Exception as e:
        print(f"UYARI: {model_name} modelinin ROC eÄŸrisi Ã§izilemedi: {e}")
        continue

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Rastgele sÄ±nÄ±flandÄ±rÄ±cÄ±nÄ±n Ã§izgisi
plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05]) # Eksen limitleri
plt.xlabel('YanlÄ±ÅŸ Pozitif OranÄ± (FPR)'); plt.ylabel('DoÄŸru Pozitif OranÄ± (TPR)'); # Eksen etiketleri
plt.title('Ã‡oklu Model AlÄ±cÄ± Ã‡alÄ±ÅŸma KarakteristiÄŸi (ROC) EÄŸrisi KarÅŸÄ±laÅŸtÄ±rmasÄ±'); # BaÅŸlÄ±k
plt.legend(loc="lower right"); # AÃ§Ä±klama
plt.grid(True)
plt.show()

# GRAD-CAM YORUMLANABÄ°LÄ°RLÄ°ÄÄ° (En Ä°yi Model Ä°Ã§in) Gradient-weighted Class Activation Mapping
print("\n## 7. KRÄ°TÄ°K: Grad-CAM YorumlanabilirliÄŸi ğŸ”")

# GRAD-CAM FONKSÄ°YONLARI

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    """Grad-CAM haritasÄ±nÄ± oluÅŸturur."""
    # Grad-CAM iÃ§in Ã¶zel bir model oluÅŸtur: GiriÅŸten son evriÅŸim katmanÄ±nÄ±n Ã§Ä±kÄ±ÅŸÄ±na ve nihai tahminlere kadar
    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])
    with tf.GradientTape() as tape: # GradyanlarÄ± hesaplamak iÃ§in GradientTape kullan
        last_conv_layer_output, preds = grad_model(img_array)
        pred_index = tf.argmax(preds[0]) # Tahmin edilen sÄ±nÄ±fÄ±n indeksini al
        class_channel = preds[:, pred_index] # Tahmin edilen sÄ±nÄ±fÄ±n skorunu al

    # GradyanlarÄ± hesapla (tahmin edilen sÄ±nÄ±f skoru ile son evriÅŸim katmanÄ± Ã§Ä±kÄ±ÅŸÄ± arasÄ±nda)
    grads = tape.gradient(class_channel, last_conv_layer_output)
    # GradyanlarÄ± ortalama (havuzlama) yaparak her filtre iÃ§in Ã¶nem aÄŸÄ±rlÄ±ÄŸÄ±nÄ± bul
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    last_conv_layer_output = last_conv_layer_output[0]
    # IsÄ± haritasÄ±nÄ± oluÅŸtur: evriÅŸim katmanÄ± Ã§Ä±kÄ±ÅŸÄ±nÄ± aÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ gradyanlarla Ã§arp
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap) # Fazla boyutlarÄ± sÄ±kÄ±ÅŸtÄ±r
    # IsÄ± haritasÄ±nÄ± normalleÅŸtir (0-1 arasÄ±na) ve negatif deÄŸerleri sÄ±fÄ±rla
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def display_gradcam(img, heatmap, alpha=0.4):
    """GÃ¶rÃ¼ntÃ¼ ve haritayÄ± birleÅŸtirip gÃ¶sterir."""
    img = (img * 255).astype(np.uint8) # GÃ¶rÃ¼ntÃ¼yÃ¼ 0-255 aralÄ±ÄŸÄ±na Ã¶lÃ§ekle
    heatmap = np.uint8(255 * heatmap) # IsÄ± haritasÄ±nÄ± 0-255 aralÄ±ÄŸÄ±na Ã¶lÃ§ekle
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) # IsÄ± haritasÄ±nÄ± gÃ¶rÃ¼ntÃ¼ boyutuna yeniden boyutlandÄ±r
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # IsÄ± haritasÄ±na renk haritasÄ± uygula
    superimposed_img = heatmap * alpha + img # IsÄ± haritasÄ±nÄ± orijinal gÃ¶rÃ¼ntÃ¼ Ã¼zerine bindir
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8) # DeÄŸerleri 0-255 aralÄ±ÄŸÄ±nda tut
    return cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB) # RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r ve dÃ¶ndÃ¼r

# Ã–rnek SeÃ§imi (Grad-CAM gÃ¶rselleÅŸtirmesi iÃ§in)
# 2 YanlÄ±ÅŸ Negatif (Kanserli ama model kansersiz dedi) Ã¶rneÄŸi seÃ§
YN_indices = np.where((Y_true_labels == 1) & (Y_pred_classes_best == 0))[0]
# 2 DoÄŸru Pozitif (Kanserli ve model kanserli dedi) Ã¶rneÄŸi seÃ§
DP_indices = np.where((Y_true_labels == 1) & (Y_pred_classes_best == 1))[0]
# 2 YanlÄ±ÅŸ Pozitif (Kansersiz ama model kanserli dedi) Ã¶rneÄŸi seÃ§
YP_indices = np.where((Y_true_labels == 0) & (Y_pred_classes_best == 1))[0]

sample_indices = []
sample_indices.extend(np.random.choice(YN_indices, min(2, len(YN_indices)), replace=False)) # YN Ã¶rneklerini ekle
sample_indices.extend(np.random.choice(DP_indices, min(2, len(DP_indices)), replace=False)) # DP Ã¶rneklerini ekle
sample_indices.extend(np.random.choice(YP_indices, min(2, len(YP_indices)), replace=False)) # YP Ã¶rneklerini ekle

plt.figure(figsize=(18, 10))
plt.suptitle(f"Grad-CAM Analizi ({best_model_name}) - Modelin Nereye OdaklandÄ±ÄŸÄ±", fontsize=16, color='darkred')

# SeÃ§ilen Ã¶rnekler iÃ§in Grad-CAM haritalarÄ±nÄ± oluÅŸtur ve gÃ¶ster
for i, idx in enumerate(sample_indices):
    img_array = np.expand_dims(X_test[idx], axis=0) # GÃ¶rÃ¼ntÃ¼yÃ¼ modele uygun formata getir
    heatmap = make_gradcam_heatmap(img_array, best_model, best_conv_layer) # Grad-CAM haritasÄ±nÄ± oluÅŸtur
    superimposed_img = display_gradcam(X_test[idx], heatmap, alpha=0.4) # Orijinal gÃ¶rÃ¼ntÃ¼ ile Ä±sÄ± haritasÄ±nÄ± birleÅŸtir

    true_label = map_characters[Y_true_labels[idx]] # GerÃ§ek etiket
    pred_label = map_characters[Y_pred_classes_best[idx]] # Tahmin edilen etiket

    # Hata tÃ¼rÃ¼nÃ¼ ve renk kodunu belirle
    if Y_true_labels[idx] != Y_pred_classes_best[idx]:
        error_type = f"HATA: YanlÄ±ÅŸ {true_label.split(' ')[0]}"
        color = 'red'
    else:
        error_type = f"DOÄRU: {true_label}"
        color = 'green'

    plt.subplot(2, 3, i + 1) # Alt grafik oluÅŸtur
    plt.imshow(superimposed_img) # BirleÅŸtirilmiÅŸ gÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
    plt.title(f"{error_type}\nTahmin: {pred_label}", fontsize=9, color=color) # BaÅŸlÄ±k ekle
    plt.axis('off') # Eksenleri kapat

plt.tight_layout(); plt.show()
print(f"**Yorum:** Grad-CAM, {best_model_name} modelinin karar verirken histopatolojik dokunun hangi bÃ¶lgelerine odaklandÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirmektedir.")

print("\n## 8. Hasta Seviyesi Nihai Uygulama (Patch YayÄ±lÄ±m Analizi) ğŸ©º")

def load_new_test_images(image_paths, width, height):
    """Etiketsiz test gÃ¶rÃ¼ntÃ¼lerini yÃ¼kler ve normalize eder."""
    X_new_test = []
    for img_path in image_paths:
        img = cv2.imread(img_path) # GÃ¶rÃ¼ntÃ¼yÃ¼ oku
        if img is None: continue # GÃ¶rÃ¼ntÃ¼ okunamadÄ±ysa atla
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR'den RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r
        img_resized = cv2.resize(img_rgb, (width, height), interpolation=cv2.INTER_CUBIC) # BoyutlandÄ±r
        X_new_test.append(img_resized / 255.0) # NormalleÅŸtir ve listeye ekle
    return np.array(X_new_test, dtype='float32')

def visualize_patient_samples(patient_patches_paths, model, patient_id, sample_count=5):
    """Belirli bir hastanÄ±n rastgele yama tahminlerini gÃ¶rselleÅŸtirir."""
    # Rastgele Ã¶rnek yama indeksleri seÃ§
    indices = np.random.choice(len(patient_patches_paths), min(len(patient_patches_paths), sample_count), replace=False)
    plt.figure(figsize=(15, 4))
    plt.suptitle(f"HASTA {patient_id} - Rastgele Yama Tahmini ({best_model_name})", fontsize=16)

    for i, idx in enumerate(indices):
        path = patient_patches_paths[idx]
        img = cv2.imread(path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)
        X_sample = np.expand_dims(img_resized / 255.0, axis=0) # Tek gÃ¶rÃ¼ntÃ¼ iÃ§in boyutu geniÅŸlet
        Y_pred_prob = model.predict(X_sample, verbose=0) # Model tahmini yap
        pred_class = np.argmax(Y_pred_prob, axis=1)[0] # Tahmin edilen sÄ±nÄ±f
        pred_label = "IDC(+) Kanserli" if pred_class == 1 else "IDC(-) Kansersiz" # SÄ±nÄ±f etiketini belirle
        color = 'red' if pred_class == 1 else 'green' # Tahmine gÃ¶re renk belirle

        plt.subplot(1, len(indices), i + 1) # Alt grafik oluÅŸtur
        plt.imshow(img_resized) # YamayÄ± gÃ¶ster
        plt.title(f"{pred_label}", fontsize=9, color=color) # BaÅŸlÄ±k ekle
        plt.axis('off') # Eksenleri kapat
    plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show()

# HastalarÄ±n Analizi (Etiketsiz Test Verileri Ä°Ã§in)
LOCAL_TEST_DIR = os.path.join(LOCAL_DIR, 'sonTest') # Yerel test dizini yolu
new_test_image_paths = []
# Test dizinindeki tÃ¼m gÃ¶rÃ¼ntÃ¼ yollarÄ±nÄ± topla
for root, _, filenames in os.walk(LOCAL_TEST_DIR):
    for filename in fnmatch.filter(filenames, '*.png'):
        new_test_image_paths.append(os.path.join(root, filename))

# Hasta ID'lerini ve yama yollarÄ±nÄ± birleÅŸtir
patient_data = [{'path': path, 'id': path.split(os.path.sep)[-2]} for path in new_test_image_paths]
unique_patient_ids = sorted(list(set([d['id'] for d in patient_data]))) # Benzersiz hasta ID'lerini al
DIAGNOSIS_THRESHOLD = 15 # Klinik teÅŸhis eÅŸiÄŸi (%15 kanserli yama oranÄ±)

final_diagnosis_results = {} # Nihai teÅŸhis sonuÃ§larÄ±nÄ± saklamak iÃ§in sÃ¶zlÃ¼k
print(f"\n--- {len(unique_patient_ids)} Etiketsiz HastanÄ±n Analizi (Model: {best_model_name}, EÅŸik: %{DIAGNOSIS_THRESHOLD}) ---")

# Her bir hasta iÃ§in analiz yap
for patient_id in unique_patient_ids:
    patient_patches_paths = [d['path'] for d in patient_data if d['id'] == patient_id] # Hastaya ait yama yollarÄ±nÄ± al
    X_patient = load_new_test_images(patient_patches_paths, IMG_WIDTH, IMG_HEIGHT) # Yama gÃ¶rÃ¼ntÃ¼lerini yÃ¼kle
    if len(X_patient) == 0: continue # GÃ¶rÃ¼ntÃ¼ yoksa atla

    Y_patient_pred_prob = best_model.predict(X_patient, verbose=0) # Model ile tahmin yap
    # Kanserli olarak tahmin edilen yama oranÄ±nÄ± hesapla
    predicted_cancer_ratio = np.sum(np.argmax(Y_patient_pred_prob, axis=1) == 1) / len(X_patient) * 100

    # TeÅŸhis eÅŸiÄŸine gÃ¶re nihai kararÄ± belirle
    final_diagnosis, color_code = ("POZÄ°TÄ°F (Kanserli)", 'red') if predicted_cancer_ratio >= DIAGNOSIS_THRESHOLD else ("NEGATÄ°F (Kansersiz)", 'green')
    print(f"  -> Hasta {patient_id}: YayÄ±lÄ±m: %{predicted_cancer_ratio:.2f} | Karar: {final_diagnosis}")
    final_diagnosis_results[patient_id] = {'ratio': predicted_cancer_ratio, 'diagnosis': final_diagnosis, 'color': color_code}
    visualize_patient_samples(patient_patches_paths, best_model, patient_id, sample_count=5) # Yama Ã¶rneklerini gÃ¶rselleÅŸtir

# HastalarÄ±n SonuÃ§larÄ±nÄ± Toplu GÃ¶rselleÅŸtirme (Bar grafiÄŸi)
if final_diagnosis_results:
    patients = list(final_diagnosis_results.keys())
    ratios = [res['ratio'] for res in final_diagnosis_results.values()]
    colors = [res['color'] for res in final_diagnosis_results.values()]
    plt.figure(figsize=(12, 6))
    sns.barplot(x=patients, y=ratios, palette=colors) # Bar grafiÄŸi Ã§iz
    plt.axhline(DIAGNOSIS_THRESHOLD, color='black', linestyle='--', label=f'TeÅŸhis EÅŸiÄŸi (%{DIAGNOSIS_THRESHOLD})') # TeÅŸhis eÅŸik Ã§izgisini ekle
    plt.title(f'Etiketsiz Test HastalarÄ±nÄ±n Tahmini Kanser YayÄ±lÄ±mÄ± ({best_model_name})'); plt.ylabel('Kanserli Yama YÃ¼zdesi (%)'); plt.xlabel('Hasta ID'); plt.legend(); plt.show()

print("\n--- ğŸ¯ Proje BaÅŸarÄ±yla TamamlandÄ± ve ZenginleÅŸtirildi ---")